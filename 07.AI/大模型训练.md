# 大模型训练

[TOC]

## 训练大型语言模型的典型数据预处理步骤是什么？

常见的管道包括：

* **下载/提取**：收集原始语料库（例如 Common Crawl、书籍）并提取为工作格式。
* **初步清理**：修复编码问题（UTF-8），删除无效字符，并分离混合语言。
* **启发式过滤**：应用简单规则来删除低质量的文本（例如非常短的行、样板、过度重复）。
* **重复数据删除**：删除重复或近似重复的文档（使用精确、模糊或语义技术）以确保多样性。
* **基于模型的过滤**：可选择使用分类器或其他模型来过滤内容（例如删除个人数据、亵渎、胡言乱语）。
* **混合和混洗**：组合来自不同来源的数据并进行混洗，以防止排序偏差。

每个阶段都会提升数据质量和多样性，这一点至关重要，因为劣质数据会显著降低模型性能（见下文）。 *原文链接：[https://developer.nvidia.com/blog/mastering-llm-techniques-data-preprocessing/]( https://developer.nvidia.com/blog/mastering-llm-techniques-data-preprocessing/ )*

## 为什么数据清理在 LLM 培训中很重要，它解决了哪些问题？

高质量的数据至关重要。正如 NVIDIA 所指出的， “数据质量差和数据量不足会显著降低模型准确率” 。数据清理可以在早期处理编码错误、格式错误的 HTML 或混合语言等问题。它还会删除不需要的内容：例如，重复文档、个人身份信息 (PII) 或有害文本，这些内容通常出现在抓取的语料库中。如果不进行数据清理，模型可能会学习到虚假或有害的模式（甚至侵犯隐私）。因此，严格的数据清理（修复 Unicode、区分语言、过滤格式错误）是至关重要的第一步。 *原文链接：[https://developer.nvidia.com/blog/mastering-llm-techniques-data-preprocessing/]( https://developer.nvidia.com/blog/mastering-llm-techniques-data-preprocessing/ ) *

## 什么是数据重复数据删除？为什么它在培训 LLM 时很重要？

去重是指从数据集中删除重复或几乎相同的示例。它“对于提高模型训练效率和确保数据多样性至关重要” 。在实践中，我们会删除完全重复的内容（相同的文档），甚至几乎重复的内容（例如相似的新闻文章）。这一点很重要，因为重复的内容会导致模型在重复的文本上过度拟合，并在看到相同的示例时浪费计算资源。通过消除重复，模型可以识别更广泛的语言，并提高泛化能力。例如，NVIDIA 的 NeMo 流水线明确包含去重功能（精确、模糊、语义），以防止过度拟合。 *原文链接：[https://cn.blockchain.news/news/optimizing-llms-enhancing-data-preprocessing-techniques]( https://cn.blockchain.news/news/optimizing-llms-enhancing-data-preprocessing-techniques )*

## 标记化在 LLM 中如何工作以及它为什么重要？

标记化将原始文本拆分成模型处理的离散单元（标记）。标记可以是单词、子单词或字符。有效的标记化*“将文本转换为模型可以理解的格式”* 。例如，子单词标记器（如字节对编码）将生僻词分解成常用词，这样即使是未见过的单词也能被表示出来。这种标准化使模型能够从固定词汇表中学习并处理多样化的输入。如果没有标记化，模型就无法将文本转换为向量。简而言之，标记化对于将文本转换为标记至关重要，使模型能够学习语言中的模式。 *原始链接：[https://www.datacamp.com/blog/llm-interview-questions]( https://www.datacamp.com/blog/llm-interview-questions )*

## 什么是字节对编码（BPE）以及它如何在 LLM 标记化中使用？

字节对编码 (BPE) 是一种源自文本压缩的子词标记算法。它迭代地合并最常用的字符对，以构建子词词汇表。例如，OpenAI 的 GPT 和 GPT-2 使用字节级 BPE：这使得它们能够用一个基集表示所有 UTF-8 字符，并合并常见的字符对。BPE 广泛应用于 Transformer（GPT-2、RoBERTa 等），以平衡词汇量和覆盖范围。通过将单词编码为子词，BPE 允许模型将罕见或未见过的单词分解成已知的部分来处理它们。这使得模型更加健壮，并有助于其更好地泛化到新文本。 *原文链接：[https://huggingface.co/course/chapter9/3?fw=pt]( https://huggingface.co/course/chapter9/3?fw=pt )*

## LLM 如何处理词汇表外（OOV）的单词或罕见词？

现代 LLM 使用子词标记化（例如 BPE 或 WordPiece）来处理 OOV 词。如果某个词不在词汇表中，它会被拆分成已知的较小子词单元。例如，一个罕见的技术术语可能会被分解成常见的前缀、词干或字符。这样，模型仍然可以通过组合子部分来处理新词并赋予其意义。子词标记化确保“即使在训练期间未见过某个词，模型仍然可以理解并基于其组成部分生成文本” 。 *原文链接：[https://www.datacamp.com/blog/llm-interview-questions]( https://www.datacamp.com/blog/llm-interview-questions )*

## 什么是嵌入层以及为什么它们在 LLM 中很重要？

嵌入层将离散的标记映射到连续的向量空间中。每个标记（单词或子单词）都会获得一个可训练的向量。这些嵌入捕捉语义关系：相似的单词具有更接近的向量。嵌入降低了输入的维度，并提供了模型可以处理的稠密表示。实际上，LLM 的第一层是嵌入层，它将每个标记转换为高维向量。这一点非常重要，因为它将模型对单词的理解建立在连续空间中，使其能够从语言数据中学习模式和含义。 *原始链接：[https://www.datacamp.com/blog/llm-interview-questions]( https://www.datacamp.com/blog/llm-interview-questions )*

## 解释 Transformer 模型中的自注意力概念。

自注意力机制使输入序列中的每个标记能够动态地关注（即加权）其他所有标记。在 Transformer 层中，对于给定的标记，模型会计算序列中所有标记的“注意力得分” 。这些得分决定了从每个标记中收集多少信息。此机制可以捕获上下文：标记的表示会更新为所有标记的加权和，并重点关注最相关的标记。重要的是，自注意力机制可以一步建模长距离依赖关系（无需递归）。简而言之， “自注意力机制……计算每个标记相对于序列中所有其他标记的注意力得分，从而使其能够捕获不受距离影响的依赖关系” 。这就是 Transformer 将语言语境化的核心。 *原文链接：[https://www.datacamp.com/blog/llm-interview-questions]( https://www.datacamp.com/blog/llm-interview-questions )*

## 为什么对于大型语言模型来说，Transformer 架构比 RNN 更受欢迎？

Transformer 之所以受到青睐，是因为它们允许大规模并行化并能高效地对长距离上下文进行建模。与按顺序处理 token 的 RNN（LSTM/GRU）不同，Transformer 使用并行自注意力机制。正如维基百科所述，Transformer *“基于多头注意力机制”*并且*“摒弃了循环”* 。这种设计意味着 token 可以并行处理（不依赖时间步长），因此在现代硬件上训练速度更快。此外，Transformer 可以更好地处理长距离上下文：一个层可以将任何 token 与另一个层关联起来。它们还能更好地扩展到大型数据集和模型规模。总而言之， “Transformer 没有循环单元，因此比早期的 RNN 需要更少的训练时间”* ，这就是所有近期的 LLM 都使用 Transformer 模块的原因。 *原始链接：[https://en.wikipedia.org/wiki/Transformer\_(deep\_learning\_architecture)]( https://en.wikipe dia.org/wiki/Transformer_%28deep_learning_architecture%29 )*

## LLM 中使用的 Transformer 架构的主要组件是什么？

Transformer 模块具有两个关键子层：(1) **多头自注意力**和 (2) **前馈网络** 。首先，将标记转换为嵌入（加上位置编码），然后自注意力层计算每个标记的上下文化表示。多头注意力允许模型同时关注上下文的不同方面。其次，每个标记的向量通过位置前馈网络（通常是具有非线性的 2 层 MLP）。残差连接和 LayerNorm 应用于每个子层（上图未显示）。这些层在堆栈中重复。总之，每个 Transformer 层都进行自注意力（混合标记信息），然后进行每个标记的前馈变换，从而构成 LLM 的骨干。 *原文链接：[https://en.wikipedia.org/wiki/Transformer\_(deep\_learning\_architecture)]( https://en.wikipedia.org/wiki/Transformer_%28deep_learning_architecture%29 )*

## 什么是位置编码以及为什么 Transformers 中需要它们？

Transformer 并行处理 token，因此它们本身没有序列顺序的概念。位置编码将 token 位置的信息注入模型。通常，每个位置都会映射到一个向量（通过正弦/余弦函数或学习到的嵌入），该向量会被添加到 token 嵌入中。这样，模型就可以区分“第一个词”和“第十个词”。正如一篇摘要所述， “Transformer 不使用循环模块，因此需要明确添加位置编码方案来保留词序” 。如果没有位置编码，模型会以相同的方式处理任何 token 的排列。因此，位置编码允许 Transformer 在训练和推理过程中利用词序和结构。 *原始链接：[https://machinelearningmastery.com/what-are-transformers/]( https://machinelearningmastery.com/what-are-transformers/ )*

## 仅编码器、仅解码器和编码器-解码器 Transformer 模型之间有什么区别？

* **仅编码器（例如 BERT）：**仅使用编码器堆栈。它双向读取整个输入并生成上下文嵌入。仅编码器模型适用于理解给定文本的任务（分类、回归）。
* **仅解码器（例如 GPT 系列）：**仅使用带有掩码自注意力机制的解码器堆栈。它以自回归的方式预测 token，因此每个 token 只能关注之前的 token。仅解码器模型专为生成（完成文本、对话）而设计。
编码器-解码器（例如 T5、BART）：完整的序列到序列 Transformer。编码器处理输入文本，解码器根据编码器的输出生成输出。这适用于翻译或摘要等输出文本依赖于输入文本的任务。

总而言之，纯编码器模型负责编码文本，纯解码器模型负责生成文本，而编码器-解码器模型则同时完成这两项任务，每种模型都会根据不同的角色调整 Transformer 模块。 *原文链接：[https://en.wikipedia.org/wiki/Transformer\_(deep\_learning\_architecture)]( https://en.wikipedia.org/wiki/Transformer_%28deep_learning_architecture%29 )*

## Transformer 中的前馈子层起什么作用？

前馈（FFN）子层是一个基于位置的两层神经网络，独立应用于每个 token 的向量。正式名称是 MLP：

```
FFN（x）= φ（x W ₁ + b ₁ ）W ₂ + b ₂
```

其中 φ 是非线性函数（例如 ReLU 或 GELU）。在实践中，这会扩展维度并对每个 token 表示应用非线性变换。例如，在 GPT 和 BERT 中，中间（隐藏）大小约为嵌入大小的 4 倍，从而允许更丰富的变换。前馈层处理每个 token 的上下文丰富的嵌入（来自自注意力机制），这对于模型的表达能力至关重要。它之后是残差连接和 LayerNorm。简而言之，FFN 在注意力机制之后为每个 token 增加了深度和非线性。 *原始链接：[https://en.wikipedia.org/wiki/Transformer\_(deep\_learning\_architecture)]( https://en.wikipedia.org/wiki/Transformer_%28deep_learning_architecture%29 )*

## 在自回归 LLM（如 GPT）的预训练期间使用什么目标？

自回归法学语言模型 (LLM) 的训练目标是：根据所有先前的标记预测下一个标记。具体来说，该模型会观察一个长文本序列，并在每个位置最大化下一个单词的实际概率。正如一篇文献所述，LLM *“优化这些 LLM，使其能够简单地预测给定序列中的下一个单词”* 。这种下一个标记预测损失（通常是交叉熵）使模型能够从大量未标记的文本中学习语法、语义和事实。实际上，这意味着在预训练期间，模型会一次生成一个标记，并进行训练以匹配真实数据，从而构建一个通用的“语言模型”。 *原文链接：[https://www.ibm.com/think/topics/instruction-tuning]( https://www.ibm.com/think/topics/instruction-tuning )*

## 在 LLM 开发中，微调与预训练有何不同？

**预训练**是在非常庞大、多样化的文本语料库上进行的，具有自监督目标（例如，下一个标记预测），从而生成具有通用语言知识的基础模型。基础 LLM 学习广泛的模式，但并不专门针对任何特定任务。另一方面， **微调**则采用此预训练模型，并在较小的、特定于任务或指令遵循的数据集上对其进行进一步训练。例如，可以对问答对、代码示例或用户提示进行微调。微调通常使用监督学习或强化学习方法来调整模型的行为。总而言之，预训练构建了一个强大的通用语言模型，而微调（可以包括指令调整或 RLHF）则使模型专门用于某些任务或使其与人类偏好保持一致。原文链接：[https://arxiv.org/abs/2307.03185]( https://arxiv.org/abs/2307.03185 )

## 什么是指令调整以及为什么在 LLM 中使用它？

指令调优是一种监督式微调，其中 LLM 以（指令，响应）对进行训练。数据集包含提示（指令或问题）及其期望输出。通过学习将指令映射到响应，模型能够更好地理解和遵循人类的指令。正如 IBM 所解释的那样，指令调优“提高了模型在遵循指令方面的总体性能，从而帮助调整预训练模型以用于实际应用。”*换句话说，它教会 LLM 进行对话并提供帮助：经过指令调优后，模型不仅知道预测下一个单词，还知道对提示给出有意义的答案。ChatGPT 和其他聊天机器人依靠指令调优模型来实现其交互行为。 *原文链接：[https://www.ibm.com/think/topics/instruction-tuning]( https://www.ibm.com/think/topics/instruction-tuning )*

## LLM 培训中的人类反馈强化学习 (RLHF) 是什么？

RLHF 是一个两步对齐程序。首先，人工注释者根据提示对模型输出进行排序或评分；然后，基于此反馈训练一个单独的“奖励模型”。然后，使用强化学习（例如 PPO）对基础 LLM 进行微调，以最大化该奖励信号。实际上，LLM 会学习生成人类偏好的输出。正如一篇摘要所述，RLHF *“使用人工反馈训练奖励模型，然后使用强化学习优化 LLM”*，以实现有用性和真实性等品质。这种方法在 InstructGPT 和 ChatGPT 中被广泛使用，使模型的响应更符合用户期望。实际上，在标准预训练之后，RLHF 会提供最后的调整步骤，根据人工判断调整 LLM 的行为。原文链接：[https://developer.ibm.com/technologies/ai/articles/what-is-instruction-tuning/]( https://developer.ibm.com/technologies/ai/articles/what-is-instruction-tuning/ )*

## 为什么大型语言模型需要分布式训练？

最先进的 LLM 拥有数十亿甚至数万亿个参数，单张 GPU 无法容纳。例如，GPT-3（1750 亿个参数）在多台服务器上约 10,000 块 Nvidia A100 GPU 上进行训练。此外，所需的计算量极其巨大（百亿亿次浮点运算/天）。分布式训练可以拆分工作：将数据和/或模型分配到多张 GPU 上。如果没有分布式，由于内存限制和不切实际的超长运行时间，训练此类模型将变得不可能。因此，数据并行（每个 GPU 都持有基于不同数据的模型副本）和模型/流水线并行（将模型拆分到多个 GPU 上）等技术被用于利用庞大的 GPU 集群。高速互连（见下文）可使数千张 GPU 保持同步。原文链接：[https://community.juniper.net/blogs/sharada-yeluri/2023/10/03/large-language-models-the-hardware-connection ]( https://community.juniper.net/blogs/sharada-yeluri/2023/10/03/large-language-models-the-hardware-connection )*

## LLM 培训中的数据并行、模型并行和流水线并行是什么？

这些是在 GPU 之间分配计算的方法：

* **数据并行性**：每个 GPU 存储完整的模型副本并处理不同的数据批次。每个批次结束后，梯度会在各个 GPU 之间进行平均。这可以加快大批次的训练速度，但无法解决单个 GPU 的内存限制问题。当模型足够小时，单靠数据并行性就可以发挥作用，但对于 LLM 来说，这通常是不够的（模型无法在一个 GPU 上加载）。
* **模型并行性**：模型本身被拆分到不同的 GPU 上。例如，不同的层（或层的张量切片）被放置在不同的 GPU 上。在前向/后向传播过程中，数据按顺序流经这些 GPU。这允许训练模型占用超过单个 GPU 内存的内存。然而，由于 GPU 必须通信中间激活，因此这会带来延迟。
流水线并行：一种模型并行形式，其中连续的层构成流水线中的“阶段”。每个 GPU（阶段）并行处理不同的微批次以提高利用率。DeepSpeed 将数据、模型和流水线并行相结合称为“3D 并行”，从而能够训练万亿参数的模型。

现代 LLM 框架通常将这三种方法结合起来：跨服务器的数据并行、服务器内的模型（张量）并行以及跨层段的流水线并行。这种混合方法是将训练扩展到极限规模的关键。

原文链接：[https://community.juniper.net/blogs/sharada-yeluri/2023/10/03/large-language-models-the-hardware-connection]( https://community.juniper.net/blogs/sharada-yeluri/2023/10/03/large-language-models-the-hardware-connection )*

## 什么是 ZeRO（零冗余优化器）系列以及为什么要使用它？

ZeRO 是一套用于数据并行训练的内存优化技术。在原始数据并行中，每个 GPU 都会存储完整的模型参数、梯度和优化器状态，这会导致大量冗余。ZeRO 将这些数据分片（拆分）到各个 GPU 上，因此每个参数张量在系统中只有一个副本。这“让您可以访问所有设备的 GPU 内存总和”*，而无需复制数据。其结果是：您可以在相同的硬件上训练更大的模型。诸如**ZeRO-Offload**之类的变体允许将部分参数或优化器状态保存在 CPU/NVMe 上，而**ZeRO-Infinity**甚至跨 GPU/CPU/NVMe 分片，以进一步扩展。总而言之，ZeRO 牺牲了通信量来消除内存浪费，在不更改模型代码的情况下显著增加了有效模型大小。原文链接：[https://www.deepspeed.ai/tutorials/large-models-w-deepspeed/ ]( https://www.deepspeed.ai/tutorials/large-models-w-deepspeed/ )*

## 什么是梯度累积以及何时使用它？

当 GPU 内存有限时，梯度累积可用于有效增加批次大小。该模型并非每个小批次都更新权重，而是依次处理多个小批次，对它们的梯度求和（累积），然后执行单个优化步骤。正如 Aman AI 所解释的那样，它会循环遍历 N 个小批次，累积梯度，然后更新一次权重。这需要额外的计算（更新之间需要更多前向/后向传递），以换取使用更大有效批次的能力。这对于 LLM 训练非常有用，因为更大的批次可以提高训练的稳定性和收敛性，但 GPU 可能无法一次性处理这些大批次。通过在 4 个小批次上累积梯度，可以模拟 4 倍大的批次，而不会超出内存限制。 *原始链接：[https://aman.ai/primers/ai/grad-accum-checkpoint/]( https://aman.ai/primers/ai/grad-accum-checkpoint/ )*

## 什么是梯度检查点以及为什么它在 LLM 培训中很有用？

梯度检查点是一种节省内存的技术。通常，在前向传播过程中，网络会保存所有中间激活值，以便它们可以在后向传播中使用来计算梯度。检查点会选择性地丢弃其中一些激活值，并在之后的反向传播过程中动态地重新计算它们。Aman AI 解释道：只有一部分激活值会被缓存，其余激活值会在需要时重新计算。这种权衡使用了额外的计算，从而大幅减少了内存使用量。对于非常深或非常宽的 LLM，检查点可以将峰值 GPU 内存减少大约一半，从而支持训练更大的模型或更长的序列。缺点是训练速度较慢，但为了将模型放入有限的 GPU 内存中，这样做通常是值得的。 *原文链接：[https://aman.ai/primers/ai/grad-accum-checkpoint/]( https://aman.ai/primers/ai/grad-accum-checkpoint/ )*

## 为什么大型语言模型要使用混合精度训练（使用 FP16 或 BF16）？

混合精度通过对大多数张量使用 16 位浮点数，同时在需要的地方保留一定数量的 32 位精度，从而加快训练速度并减少内存占用。NVIDIA 报告称，在现代 GPU 上，混合精度比 32 位精度快 *1.5 到 5.5 倍* 。例如，GPT-3（1750 亿个参数）在 1024 个 A100 GPU 上使用混合精度估计需要约 34 天，而使用完整的 FP32 精度则需要一年多。这种显著的加速几乎不会对模型质量造成任何损失（使用了损失缩放等技术来保持稳定性）。实际上，几乎所有最先进的 LLM 都使用 FP16 或 BF16 进行训练，以便在现有硬件上进行训练。原文链接：[https://pytorch.org/blog/what-every-user-should-know-about-mixed-precision-training-in-pytorch/]( https://pytorch.org/blog/what-every-user-should-know-about-mixed-precision-training-in-pytorch/ )*

## 为什么在训练大型神经网络时经常应用梯度裁剪？

梯度限幅会限制梯度向量的大小（例如，通过最大范数），以防止*梯度爆炸* 。如果梯度过大，权重更新可能会破坏训练的稳定性。限幅可以使梯度保持有界。例如，Hugging Face 训练工具包设置了默认的“max_grad_norm”以防止梯度爆炸。在深度模型或高学习率方案中，限幅可确保稳定收敛。在 Transformer 中，梯度限幅（通常按范数）是保持训练稳定的标准做法，尤其是在最初几次迭代中。正如 HF 所指出的， *“对于防止反向传播过程中梯度爆炸至关重要”* 。 *原始链接：[https://huggingface.co/docs/autotrain/en/llm\_finetuning\_params]( https://huggingface.co/docs/autotrain/en/llm_finetuning_params )*

## 通常使用哪些指标来评估 LLM 在语言任务上的表现？

评估取决于具体任务：常用指标包括语言建模的困惑度 (perplexity) ，以及下游任务的特定分数（准确率、F1 等）。正如某资料来源总结的那样，关键指标包括：

* **困惑度**：衡量模型预测保留文本的准确程度（值越低越好）。它是真实下一个标记的指数平均负对数似然。
* **准确度/F1**：用于分类或标记任务（例如情感分析、NER）来衡量正确性。
* **BLEU/ROUGE**：用于生成任务（如翻译或摘要），以量化模型输出和参考文本之间的 n-gram 重叠。

也可以使用其他指标，例如精确匹配、EM 或专门的评估器（例如 BLEURT）。开放式生成通常采用人工评估。但在大规模预训练中，验证集上的*困惑度*是监控训练进度的标准。

原文链接：[https://www.datacamp.com/blog/llm-interview-questions]( https://www.datacamp.com/blog/llm-interview-questions )*

## LLM 评估中使用 BLEU 和 ROUGE 指标来做什么？

BLEU 和 ROUGE 是用于比较生成文本与参考文本的自动指标。BLEU （双语评估替代指标）衡量 n-gram 的准确率：即模型输出中有多少部分出现在参考文本中。它通常用于机器翻译。ROUGE （面向召回率的替代指标）衡量 n-gram 或子序列的召回率，常用于摘要和对话。分数越高，与参考文本的重合度就越高。它们可以粗略地量化反馈生成质量。例如，BLEU 越高，表示模型的翻译与参考文本的匹配度越高。这些指标存在局限性（它们无法完全捕捉语义），但它们仍然是生成型 LLM 任务的基准测试标准。 *原文链接：[https://www.datacamp.com/blog/llm-interview-questions]( https://www.datacamp.com/blog/llm-interview-questions )*

## 在 LLM 培训中过度拟合是什么？为什么它值得关注？

当模型记忆训练数据而不是学习泛化时，就会发生过拟合。在法学硕士 (LLM) 中，这可能意味着模型在不理解的情况下复现精确的训练短语。风险在于，模型在类似训练的数据上表现良好，但在新输入上表现不佳。这还可能导致模型意外地记住训练集中的敏感内容或受版权保护的内容。结果，模型可能会重复训练中发现的事实错误或泄露私人信息。良好的泛化能力至关重要，因此在实践中，需要监控验证困惑度 (perplexity)，并在过拟合变得严重之前停止训练。正则化和 dropout 等技术也可用于缓解过拟合。简而言之，法学硕士 (LLM) 中的过拟合意味着模型“记忆”了过多的训练文本，而不是学习广泛的语言模式。原文链接：[https://www.nccgroup.com/us/research-blog/exploring-overfitting-risks-in-large-language-models/]( https://www.nccgroup.com/us/research-blog/exploring-overfitting-risks-in-large-language-models/ )*

## 语言模型的缩放定律告诉我们如何有效地使用计算？

缩放定律（Kaplan 等人，2020）表明，模型性能（交叉熵损失）与模型大小、数据集大小和计算量呈幂律关系。至关重要的是，更大的模型更“计算高效” ：在计算预算相同的情况下，用更少的数据训练更大的模型通常比用更小的模型训练收敛效果更好。事实上，作者得出的结论是，最优训练会将大部分计算资源分配给一个非常大的模型，并在“收敛之前显著”停止训练。实际上，这意味着我们应该训练大型模型更少的训练次数，而不是完全训练小型模型。这些定律有助于预测添加参数或数据将如何改善结果，并指导训练资源的有效分配。 *原文链接：[https://arxiv.org/abs/2001.08361]( https://arxiv.org/abs/2001.08361 )*

## 什么是 LoRA（低秩自适应）以及为什么它用于微调 LLM？

LoRA 是一种参数高效的微调方法。它不会更新所有模型权重，而是“冻结”预训练权重，并在每一层中注入小型可训练的低秩矩阵。例如，LoRA 可能会向每个权重矩阵添加两个秩为𝐫 的矩阵（大小分别为 d×r 和 r×d）。这大幅减少了可训练参数：DeepMind 报告称，训练的参数减少了多达 10,000 倍，而使用的 GPU 内存减少了约 3 倍。在微调期间，仅更新这些低秩矩阵。其优势在于，微调速度更快，占用的内存更少，但性能仍然与完全微调相当。总而言之，LoRA 允许通过训练少量额外参数，以极低的成本将大型 LLM 应用于新任务。原文链接：[https://arxiv.org/abs/2106.09685]( https://arxiv.org/abs/2106.09685 )

## LLM 培训中的“早停”（early stops）是什么意思，为什么它可能是最佳的？

缩放定律结果表明，将模型完全训练至收敛通常效率低下。相反，应该“尽早”停止——即在验证损失改进减少后停止——以节省计算资源。Kaplan 等人发现，最佳训练“涉及使用相对较少的数据量训练非常大的模型，并在收敛前显著停止。”* 。这可以避免为了边际收益而浪费资源。在实践中，这意味着监控验证性能，并在出现过拟合或损失趋于平缓时停止训练。尽早停止可确保在给定的计算预算下，我们能够充分利用它（通常是将更多步骤分配给较大的模型，而不是完全训练较小的模型）。 *原文链接：[https://arxiv.org/abs/2001.08361]( https://arxiv.org/abs/2001.08361 )*

## 哪些技术可以降低大型模型的计算成本（例如修剪、量化）？

有几种方法可以提高 LLM 的效率：

* **修剪**：从训练模型中删除不太重要的权重或整个神经元，从而减少尺寸和推理成本。
* **量化**：将权重和激活转换为较低精度（例如 8 位甚至 4 位）以进行推理，通常将准确度损失降至最低。
* **知识提炼**：训练一个规模较小的“学生”模型，以模拟规模较大的“教师”法学硕士 (LLM) 的输出。学生模型的运行速度会更快。
* **稀疏架构**：使用稀疏注意力或混合专家层，其中每个输入仅激活模型的部分。

正如一篇摘要指出的那样，诸如剪枝、量化和蒸馏之类的技术“可以显著降低LLM的大小和推理成本” 。这些方法以牺牲部分模型容量为代价来降低计算/内存成本，这对于在资源受限的环境中部署LLM至关重要。 *原文链接：[https://www.datacamp.com/blog/llm-interview-questions]( https://www.datacamp.com/blog/llm-interview-questions )*

## 在法学硕士 (LLM) 的背景下，知识提炼是什么？

知识蒸馏会训练一个较小的（学生）模型来复制较大的（教师）法学硕士 (LLM) 模型的行为。教师模型生成输出（通常是基于 token 的概率分布），然后训练学生模型来匹配这些输出。这将“知识”从大模型迁移到小模型。其好处是，学生模型可以用更少的参数和更快的推理速度实现类似的性能。蒸馏通常在训练一个非常大的模型后使用：通过将其蒸馏成一个紧凑的模型，可以保留其大部分准确率，同时大大降低计算/内存需求。实际上，学生模型从教师模型的“软”目标中学习。如上所述，蒸馏是“降低推理成本”和创建轻量级语言模型 (LM) 的关键方法之一。 *原文链接：[https://www.datacamp.com/blog/llm-interview-questions]( https://www.datacamp.com/blog/llm-interview-questions )*

## 通常使用什么硬件（GPU、TPU）来训练大型语言模型？

LLM 通常在具有大内存的尖端加速器上进行训练。例如，GPT- 3 在多台服务器上约 10,000 个 Nvidia A100 GPU 上进行训练。Meta 的 LLaMA 使用了 16,000 个 A100 GPU 集群（采用 InfiniBand 架构）。此外，TPU 也被使用（例如，谷歌的 PaLM 在数千个 TPU v4 芯片上进行训练）。这些设备具有高带宽内存 (HMB) 和互连（GPU 上的 NVLink 或 NVSwitch）。像 A100（40/80GB HBM）和 H100（80GB）这样的较新的 GPU 很常见，NVIDIA 的 NVLink/NVSwitch 架构也可用于连接节点内的 GPU。总而言之，训练通常使用数百到数千个顶级 GPU 或 TPU，它们通过高速链路联网，以处理模型并行性和大数据吞吐量。原文链接：[https://developer.nvidia.com/blog/large-language-models-hardware]( https://developer.nvidia.com/blog/large-language-models-hardware )*

## 为什么高带宽互连（NVLink、InfiniBand）在 LLM 培训中至关重要？

训练是分布式的，因此 GPU 之间的快速通信至关重要。在服务器内部，GPU 通过 NVLink 或 NVSwitch 连接，从而能够以约 600 GB/s 的速度共享梯度和激活函数。服务器之间使用高速网络，例如 InfiniBand（例如 Meta 的 Quantum-2 25.6 Tbps IB）或 400Gb 以太网。这些互连最大限度地减少了数据并行中同步梯度或在模型并行中传递张量的时间。如果没有高带宽，GPU 将处于空闲状态，等待数据交换。正如 NVIDIA 指出的那样，LLaMA 训练涉及“GPU 之间使用 NVSwitch，节点之间使用 400Gbps InfiniBand” 。因此，要扩展到数千个 GPU，超高速网络对于保持模型所有部分同步并实现高吞吐量至关重要。原文链接：[https://developer.nvidia.com/blog/large-language-models-hardware]( https://developer.nvidia.com/blog/large-language-models-hardware )*

## 什么是学习率预热计划，为什么在训练 Transformers 时使用它？

预热计划会将学习率从非常低的阶段开始，并在初始阶段逐渐将其提升至目标值。例如，10% 的预热意味着学习率 (LR) 在前 10% 的训练步长内线性上升。这种技术可以防止在训练开始时出现较大的梯度下降，从而避免模型不稳定。在 Transformers 中，预热尤其有用，因为初始权重（通常来自随机或通用初始化）尚未调整。正如一个配置说明所述，预热阶段在开始时“增强了模型稳定性” 。预热之后，通常会应用衰减计划。在实践中，几乎所有 LLM 训练方案都使用预热（通常占总步长的 0.1 或 0.01），以确保初始训练的顺利进行。 *原始链接：[https://huggingface.co/docs/autotrain/en/llm\_finetuning\_params]( https://huggingface.co/docs/autotrain/en/llm_finetuning_params )*

## Transformers 中常用哪些激活函数？

Transformer 的原始论文使用了 ReLU，但大多数现代 LLM 使用了更平滑的激活函数。GELU（高斯误差线性单元）是一种常见的选择。例如，GPT-1 和 BERT 在其前馈层中使用了 GELU。较新的模型（例如许多 GPT-3 变体、LLaMA 和 PaLM）采用了称为SwiGLU或类似（ Swish 的变体）的门控线性单元。这些激活函数往往比原始 ReLU 更能提高训练稳定性和性能。总而言之，GELU 及其门控变体是当今 LLM 的标准，取代了较老的 ReLU。 *原始链接：[https://en.wikipedia.org/wiki/Transformer\_(deep\_learning\_architecture)]( https://en.wikipedia.org/wiki/Transformer_%28deep_learning_architecture%29 )*

## 为什么 Transformer 模型要使用层归一化？

层归一化 (LayerNorm) 用于稳定和加速训练。它对每个 token 特征的隐藏激活进行归一化。虽然 LayerNorm 对表征能力并非严格要求，但在深度 Transformer 网络中，“LayerNorm ”*“对于数值稳定性和收敛性至关重要” 。通过减少内部协变量偏移，它有助于梯度更好地流动。原始 Transformer 在每个子层（在注意力机制/FFN 之前或之后）应用 LayerNorm 以避免训练困难。在实践中，未经任何归一化的 Transformer 通常无法训练或需要非常仔细的调参。因此，LayerNorm 是确保深度 LLM 可靠训练的一个虽小却至关重要的组成部分。 *原文链接：[https://en.wikipedia.org/wiki/Transformer\_(deep\_learning\_architecture)]( https://en.wikipedia.org/wiki/Transformer_%28deep_learning_architecture%29 )*

## 为什么 Transformer 模型使用残差（跳过）连接？

残差连接有助于避免梯度消失问题，并支持训练非常深的网络。在 Transformer 中，每个子层（注意力机制或 FFN）将其输入添加到其输出： *y = F(x) + x* 。这意味着梯度可以直接流经恒等路径，从而保留原始信号。因此，即使子层的变换 F(x) 较小或不稳定，网络也能通过跳跃保留原始表示。这种设计*“旨在避免梯度消失问题并稳定训练过程”* 。本质上，残差使我们能够堆叠多层（数百层或更多）并仍然有效训练，这对于大型 LLM 至关重要。 *原始链接：[https://en.wikipedia.org/wiki/Transformer\_(deep\_learning\_architecture)]( https://en.wikipedia.org/wiki/Transformer_%28deep_learning_architecture%29 )*

